# Falcon Project for 2025 NLLP

## checklist

### SOTA case search
- [v ] **최근 5년 내 사례 조사:** LexGLUE ECtHR Task A 데이터를 활용하여 **사실 검증**(fact verification)을 수행한 논문이 2020~2025년에 존재하는지 조사하기  
- [ ] **관련 최신 연구:** (추가 조사 중)

### LexGLUE ECtHR Task A 데이터 이해
- [v ] **레이블 정보 부록 추가:** 
   1. 가능한 레이블(위반 조항 번호) 10개의 정의를 부록에 정리  
   2. 판사의 판결을 위해 필요한 **요건사실** 정리
- [v ] **데이터 샘플 별 레이블 경우의 수 파악:** 
   1. **레이블 1개** – 위반한 조항이 딱 하나인 경우  
   2. **레이블 2개** – 두 개의 조항 위반  
   3. **레이블 빈 리스트** – (이는 ... 현재 미정)

### 관련 모델 이해 (LegalBERT, Llama 등)
- [v ] **도메인 특화 모델 학습:** LegalBERT 등의 법률 특화 언어모델의 아키텍처와 특성 파악  
- [v ] **범용 LLM 조사:** Llama3 등 최신 대형 언어모델의 법률 분야 적용 사례 조사
- [v ] longformer

### 전처리 (Preprocessing)
- [ ] **데이터 대화형 변환:** LexGLUE ECtHR Task A의 `{text, label}` 형식을 **대화 맥락의 주장 검증(NLI)** 용 `{context, claim, label}` 형식으로 변환  
- [ v] **Supported 케이스 작성:** `Supported` 레이블에 해당하는 claim을 원문에서 추출. 각 케이스당 주장에 대응하는 **근거 문장** 5개 후보를 선정하고, 무작위로 1개의 근거를 선택하여 대화의 evidence로 사용  
- [ ] **Refuted/NEI 생성 방법 정의:** `Refuted`와 `NotEnoughInfo (NEI)` 레이블의 생성 기준 정의  
   - **Refuted:** 증거 텍스트에 해당 주장을 **명시적으로 반박**하는 내용이 존재해야 함 (주장과 상충되는 내용이 직접적으로 나타나는 경우)
   - **NEI:** 텍스트에 그 주장을 **입증하거나 반박할 정보가 전혀 없는 경우** (텍스트가 해당 주장에 대해 침묵하거나 정보가 불충분한 상태)
   - **NotEnoughInfo:** 위키피디아 기반 팩트체크(Wikipedia FEVER, 2018)에서 사용된 개념으로, 주장 뒷받침/반박 정보 자체가 부재함을 의미
- [ ] *(WIP)*

## Timeline

- **W1 (~7.19)** – ACL 논문 구조 초안 작성 및 관련 연구 정리, 연구 질문 확정, 모델/데이터 선정, 실험 디자인 초안 설계  
  - *Research Question 1:* 법률적 추론 구조에 기반하여 판결문의 문단들을 클러스터링하고, 이 중 **주장과 밀접한 핵심 문단만 선별**하여 입력할 경우, 문서 전체를 사용할 때보다 팩트체크(주장 검증) 성능이 향상되는가?  
- **W2 (~7.25)** – 주요 실험 3개 진행, 실험 로그 및 시각화 수집, Introduction 및 Related Work 초안 작성  
- **W3 (~8.1)** – 추가 Ablation 실험 및 결과 확보, Results & Analysis 정리, Methodology 작성 (성능 비교 표와 그래프 등 시각화 포함)  
- **W4 (~8.8)** – Abstract, Conclusion 작성 및 References 정리, 논문 초안 완성 준비

## Data Transformation

- **Dataset:** LexGLUE – ECtHR Task A (European Court of Human Rights 판결문 데이터셋)  
- **Data Fields:** 각 샘플은 `text` (판결문 본문)와 `label` (위반 조항 리스트)로 구성됨.  
- **Dialogue 변환 방법:** 긴 판결문 `text`를 대화용 맥락과 주장으로 분리하여 `{context (evidence), claim}` 쌍으로 만들고, 원래 `label`을 해당 claim의 판정 레이블로 활용하는 방안을 검토 중.  
  - **Conversion Approach:** `text`에서 **주장(statement)** 부분과 **근거(context)** 부분을 분리하여 대화 형식으로 구성. `label`은 해당 **주장의 판정 결과** (Supported/Refuted/NEI)로 매핑.  
  - **예시 (구상 중):** 원본 JSON 샘플 → 대화 형식 변환 샘플 (예: 판결문 내용을 대화체로 변환하여 판사와 원고/피고의 발화로 구성).  
  - **고려 사항:** 단순히 본문 `text`를 대화로 변환하는 것만으로 “대화 속 주장 팩트체크 성능 향상”을 입증하기에는 부족할 수 있음. 대화체 변환의 정당성을 보이기 위해 추가 실험 설계가 필요하다.  
  - **Turn-Level 대화 구조:** 판결문을 여러 **턴(turn)**의 대화로 분할하는 방법도 고려 (예: 판사 질문 ↔ 원고 답변 형태로 사실관계를 재구성).

### LexGLUE ECtHR Task A 관련 추가 조사

1. **긴 문서 처리 한계:** ECtHR 판결문은 매우 길어서 **모델 입력 한계**를 초과할 수 있다.  
2. **핵심 정보 식별 어려움:** 긴 텍스트에서 **주장 검증에 필요한 핵심 문맥**을 식별하기 어렵다.  
3. **도메인 특수성:** 법률 도메인의 전문 용어와 지식 격차로 일반 모델이 이해하기 힘들다.  
4. **벤치마크 한계:** LexGLUE는 유럽권 법률에 국한되므로 국가별 법체계 차이가 존재하고, 다양한 법률 데이터에 대한 검증이 필요하다.

위 문제들 중 1번과 2번은 **문맥 전처리/요약 기법**으로 해결할 수 있을 것으로 기대한다. 요약한 핵심 문맥만 사용하면 모델 입력 길이 문제를 완화하고, 중요한 정보에 집중할 수 있다. 이러한 요약 기법이 실제로 팩트체킹 성능을 향상시키는지 검증해야 하며, 최적의 요약 방식에 대한 추가 연구도 필요하다.

## 실험 계획

연구의 방법론은 다음 **5단계**로 구성된다: 문단 임베딩 → 클러스터링 → **주장**과 유사한 클러스터 선택 → 선택 클러스터 내 상위 중요 문단 3개 선별 → NLI 입력 구성 및 모델 판정.

또한, 아래와 같은 여러 실험 조건을 비교할 계획이다:

1. **전체 문맥 사용 (Baseline):** 케이스의 **모든 문단**을 한 번에 입력하여 판단하는 베이스라인 모델 (예: Longformer, or fine-tuned Llama2 etc.).  
2. **문단 선별 기법 적용:** 위의 문단 임베딩 **클러스터링을 통해 선별된 핵심 문단들만** 입력하여 NLI 기반 사실 검증 수행. 전체 대비 성능 향상을 검증.  
3. **모델 비교:** 법률 도메인 특화 모델(예: LegalBERT) vs. 범용 대형 언어모델(예: Llama2)의 성능 비교.  
4. **입력 구성 비교:** 다양한 입력 구성에 따른 성능 비교: 대화 문맥 + 증거 + 주장 **vs.** 비대화 문맥 + 주장 **vs.** 증거(요약된 문맥) + 주장 (DialFact 데이터 설정 참고).  
5. **평가지표:** 정확도(Accuracy), 매크로/마이크로 F1 등 **NLI 분류 성능** 전반에 대한 평가. 필요 시 law-specific metric 고려.  
6. **최종 목표:** 장문 맥락에서 **핵심 근거 추출을 통한 주장 검증 성능 향상**을 입증. (특히 법률 도메인에서 기존에 시도되지 않은 새로운 접근이라는 점 강조.)

## Structure

- **목표:** 대화 문맥 중 **주장 검증**, 즉 최종 판결에 핵심이 되는 정보만 선별하여 모델 입력으로 사용했을 때의 효과 분석.  
- **문맥 필터링 방식:** 법률적 **추론 과정**에 기반한 문단 **클러스터링**을 통해 중요 문맥을 자동 선별.  
- **적용 도메인:** 법률 **대화** 도메인 (판결문을 대화 형태로 재구성).  
- **문맥 구조:** 판결문을 **대화형 문맥**으로 변환 (예: 판사와 당사자의 문답 형식).  
- **주장 검증 방식:** 판결에 필요한 선별된 문맥만을 증거로 삼아 **NLI 모델**로 claim 검증 (Supported/Refuted/NEI 판단).  
- **연구 초점:** 법률적 추론 기반 문맥 선별이 **모델의 주장 검증 성능을 향상**시키는지 확인.

## 연구 기여점

1. **도메인 차별성:** 일반 도메인이 아닌 **법률 도메인** (특히 법률 *대화체* 기반)의 주장 검증 실험을 시도한다.  
2. **문맥 구조 차별성:** 판사처럼 **절차적 추론 흐름 구조**를 반영하여, 판결에 도움이 되는 문맥을 선별하고 활용한다.  
3. **문맥 선별 방식 차별성:** 기존 TF-IDF 혹은 학습된 모델 기반 증거 선택이 아닌, **법적 추론 과정 기반의 정보 선별 방식**을 제안한다.

### 데이터 특징 (판사의 판단 흐름에 따라 분류 가능한 이유)
이 LexGLUE ECtHR Task A 데이터의 판결문은 **사실관계 → 주장 → 판단**의 시간적/논리적 흐름으로 서술된다. 다시 말해, 해당 데이터는 판사가 판단을 내리는 **추론 구조**를 내포하고 있다.

### 판사의 추론 구조란?
실제 유럽인권재판소(ECtHR)나 한국 법원의 판결문은 보통 다음과 같은 **절차적 구조**를 따른다:

1. **사건의 배경 설명** (사실관계 파악)  
2. **당사자의 주장 정리** (무엇이 쟁점인지 정리)  
3. **법적 쟁점 제시** (적용될 법 조항이나 쟁점사항 명시)  
4. **사실 판단** (쟁점이 되는 사실에 대한 인정 또는 불인정)  
5. **법적 평가** (적용 법규에 따른 판단 근거 제시)  
6. **최종 판단** (결론적으로 법 조항 위반 여부 판단)

따라서, 본 연구에서는 판결문 전체 문맥을 크게 **{ 사건 개요, 주장 요지(쟁점 정리), 법적 평가 및 판단 }**의 세 부분으로 나누어 볼 수 있다. 이러한 구조를 활용하여 각 부분에서 **주장 검증에 핵심적인 문단**을 선별하고자 한다.

## Reference

- **Chamoun et al. (EMNLP 2023 Workshop)** – *“Automated Fact-Checking in Dialogue: Are Specialized Models Needed?”*  
  *Chamoun* 등의 연구에서는 **retrieval 방식**만 수정하여 기존 FEVER 모델을 대화 데이터에 적용하였다. **인용 이유:** 이 논문의 저자들은 **지시대명사(reference resolution)** 문제 해결이 팩트체킹 성능 향상에 중요함을 언급하며, 대화 → 단문 문서 형식으로 변환해도 FEVER 기반 모델로 성능 평가가 가능하다고 주장한다. 이 결과는 본 연구에서 **ECtHR 판결문을 대화의 문맥+증거로 활용**하는 접근에 대한 정당성을 시사한다 (대화 형태 변환 vs. 긴 원문 그대로 사용 여부에 대한 고민과 연결됨).

- **Kryściński et al.** – *“FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations”*  
  (요약 문장 **사실성 평가**를 위한 그래프 기반 접근; 관련 아이디어 조사 예정)
