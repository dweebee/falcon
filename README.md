# 논문 아이디어
### 기존 연구 문제
- 법률 도메인에서 대화형 주장검증 연구 부족
- 실제 법 위반여부 판단 시, 요건사실표를 참조하는 판사의 추론 과정에 착안해 lexglue ecthr task a의 factual paragraphs를 요건사실표를 참조해 요약, 주장과 함께 검증 시 모델의 주장 검증 성능 증가 기대
- 많은 문맥 중, 주장 검증 성능을 해치는 불필요한 문맥이 존재함. 예를 들어, lexglue의 이런 데이터 샘플을 보면, 문맥에서 applicant의 가족관계가 나오는데, claim은 "이 사건에서 표현의 자유가 침해됨."이라는 주장을 검증할 때, applicant의 가족관계나 학업성취같은 내용은 주장검증에 도움이 안될 수 있음. James Thorne 도 주장검증에 해로운 증거를 제외하면 오히려 줄어든 문맥으로 사실검증 성능이 강화함을 주장했었음. 
### 아이디어 제안
- 법률 도메인 대화에 사실검증을 적용하는 의의
- 새로운 데이터 제안: 대화형 법률 위반여부 주장 사실검증
- 이 연구의 기여점
- 	

### 실험설계
- 관련 데이터 및 모델 정리
	- **Dialfact**
		- 위키피디아 근거와 쌍을 이루는 22,245개의 대화형 주장을 모은 벤치마크로, 대화 응답의 사실 여부를 Supported/Refuted/Not Enough Info로 분류하는 과제
		- 입력:출력 = {evidence+context[-2:]+claim}:{S/R/N}
	- **LexGLUE ECtHR Task A**
		- 입력:출력 = `text(factual paragraphs)`:`label(1개 이상의 위반된 조항번호로 구성된 리스트)`
		- LexGLUE는 다양한 법률 NLP 태스크에 대한 벤치마크 데이터셋 모음으로, 여러 법률 분야 과제에 모델을 평가하기 위한 표준 데이터셋을 제공. 
	- **가명**
		- 입력:출력 = {context+claim}:{S/R/N}
- 모델
	- legalbert: 법률 도메인 사전학습 모델
- lawcasebert
	- llama3

### 실험
1. 환경 설정 및 데이터 준비
	- 깃헙 코랩 GPU
 	- LexGLUE ECtHR Task A 데이터 가공 및 저장
  	- Baseline Exp.: Legalbert + {lexdialfact}
   		- 하이퍼파라미터 Tensorboard 저장
     	- train/valid/test 성능 저장
		- 오탐 분석
   - Idea Exp.: Legalbert + {lexdialfact_corectx: 문맥요약된 데이터}
		- Ch4.Methods 상세 기술
		- 베이스라인과 동일 조건에서 실험!!!
		- 하이퍼파라미터 튜닝해서 실험(튜닝과정도 기록)
		- train/valid/test 성능 저장
   - 실험결과 표 제작: 베이스라인, 제안모델의 지표별 비교
   		- 결과를 시각화(그래프나 그림) in 부록
     	- 표 검증!!! "재현해서 연속 3회실험 평균+-표준편차" 표에 반영

---
### 논문 초안
##### Abstract
- After feedback
##### Ch1. Introduction
- 연구 배경과 문제제기
- 본 연구의 목적과 기여를 소개. 예를 들어 "대화형 인공지능에서 사실검증은 중요한 문제이나 기존 연구는 X의 한계가 있었다. 본 논문에서는 Y 기법을 제안하여 이러한 한계를 개선하고자 한다"
- 전체 내용을 관통하는 연구 질문, 해결하고자 하는 문제, 그리고 기여점

##### Ch2. Related Work
- 연구와 연관된 선행 연구들을 소개
- 대화 속 사실 검증 개념과 관련된 연구들을 정의하고 인용 (예: DialFact로 대표되는 대화형 fact-checking 과제 정의, 기존 사실 검증 데이터셋들 FEVER, LIAR 등 간략 언급).
- 법률 도메인에서의 NLP 연구들. 특히 본 연구와 직접 관련된 **요건사실표** 개념을 설명하고, 그것이 법적 추론에 얼마나 중요한지 강조하세요.
	- 국내 법률 AI 연구나 판례 분석에서 요건사실표를 활용한 사례도 언급
- LexGLUE와 같은 법률 언어 이해 벤치마크에 대해 간략히 소개
	- LexGLUE는 여러 법률 NLP 태스크를 표준화해 제공하는 벤치마크로, 법률 분야에서 사전학습된 모델들이 일반 모델보다 일관되게 향상된 성능을 보였다는 결과.
- 이러한 관련 연구들을 서술하며, 우리 연구의 차별점이 무엇인지 (예: 기존에 결합되지 않은 대화 사실검증과 법률 도메인의 접목) 부각!!!

##### Ch3. Task and Dataset
- 문제 정의와 사용한 데이터셋을 설명
	- 과제 정의. 예를 들어 "본 연구의 과제는 법률 문맥의 대화에서 제기된 주장에 대해 사실 여부를 판단하는 것"과 같이 기술. 그리고 이 과제를 풀기 위해 어떤 데이터를 사용했는지?
 - LexGLUE 벤치마크 중 특정 데이터를 활용해 DialFact 형식의 (context, claim, label) 데이터셋을 구축했다면, 그 과정을 설명.
 	- 원본 LexGLUE 데이터에서 **맥락(context)**과 **주장(claim)**을 어떻게 추출/생성했는지, **레이블(label)**은 어떻게 부여했는지 기술합니다. (현재 부록의 표에 있는 10개 조항별 요건사실표와 후보 문장 50개가 이 부분에 해당하므로, 본문에서는 간략히 서술하고 표는 부록)
  	- 데이터 전처리 기법도 정리. 예를 들어 "대화 발화에서 법률 조항에 관련된 문장을 추출하여 claim 후보를 생성한 뒤, 요건사실표 상의 문장들과 대조하여 최종 claim을 선택" 등의 과정을 썼다면 이를 쓰고, 데이터 증강이나 필터링 등이 있다면 포함.
- 마지막으로, 모델에 투입하기 전 데이터 통계(예: 총 몇 개의 샘플, 클래스별 분포, 문장 길이 평균 등).
  
##### Ch4. Method
- 연구 방법론과 사용한 모델의 구조를 상세히 설명
	- 만약 베이스라인 모델과 제안 모델 함께 설명, 베이스라인은 간략히 언급하고 주요 초점은 제안 기법에 맞춥.
	- 우선 모델 개요를 소개. 예를 들어 "우리의 모델은 사전학습언어모델(예: Legal-BERT)을 기반으로, 두 가지 입력(대화 맥락과 주장)을 받아 사실 여부를 예측하는 분류기이다."처럼 큰 틀을 설명.
 - 제안하는 개선 요소를 집중적으로 기술합니다. 요건사실표를 통해 문맥을 요약.
 - 훈련 방법(loss 함수, 옵티마이저 등)이나 세부 설정(예: 클래스 불균형 대응을 위해 가중치 적용) 등 서술.
 - 어떻게 문제를 풀었는지 재현 가능할 정도로 이해해야 하므로, 구현한 알고리즘의 중요한 부분은 빠짐없이

##### Ch5. Experiments
- 실험 설정을 먼저 명시
	- 사용한 환경(예: Python 버전, 라이브러리 버전), 하드웨어(GPU 사양 등 간략히) 그리고 평가 지표와 실험 절차를 기술.
- 실험 결과표(4장에서 만든 표들)를 순차적으로 제시하면서 각 결과를 설명. 예를 들어 "표 1은 베이스라인과 제안 모델의 전체 성능 비교를 보여준다. 제안 모델이 정확도 X%로 베이스라인보다 Y%p 높게 나타났다."처럼 서술. 또한 각 결과에 대해 간략한 해석을 곁. "우리 기법 적용 시 특히 Refuted 클래스에서 성능 향상이 두드러지는데, 이는 어텐션 메커니즘이 해당 클래스에서 중요한 단서를 잘 포착한 결과로 보인다." 등의 분석을 덧붙
- 베이스라인과 통계적으로 유의미한 차이가 있는지 언급할 필요가 있다면 간단히 추가하고, 결과 전반을 요약하면서 우리 방법의 효과를 정리.
- 이 장은 결과 그 자체에 집중하고, 심도 있는 분석은 다음 장

##### Ch6. Results and Analysis
- 결과를 토대로 한 심층 분석
- 전체적인 결과 요약과 함께, 왜 그런 결과가 나왔는지 추가적인 설명. 예를 들어, 제안 기법이 특정 상황에서 강점을 보였다면 그 이유를 추론.
- 또한 가능하다면 어텐션 가중치 시각화나 모델 내부 분석을 수행. 각 사례에서 모델이 어디를 중점적으로 보고 있는지 추출 가능하다면, 몇 가지 예시를 뽑아 어텐션 가중치를 열지도 혹은 강조표시된 텍스트 등으로 보여줄. 이를 통해 모델이 요건사실표의 어떤 요소를 활용하고 있는지 설명하면 설득력
- 그리고 멀티-hop claim에 대한 평가가 포함됐다면, 한 가지 주장이 여러 문맥 증거를 필요로 하는 경우를 따로 분석합니다. 예컨대 "멀티홉이 필요한 20개의 샘플 중 우리 모델은 18개를 맞췄으나 베이스라인은 15개 맞췄다. 오답 사례를 보니 베이스라인은 두 번째 증거를 충분히 활용하지 못하는 경향을 보였다." 등 구체적인 분석을 합니다.
- 마지막으로, 오류 사례 분석도 짤막하게 포함하세요. 모델이 틀린 예시를 한두 개 제시하고 왜 틀렸는지 (예: 주장 문장이 모호했다든지, 법률 지식을 요했는데 부족했다든지) 논의.
- 이 장의 목적은 단순히 결과 숫자 이상으로 연구의 발견과 통찰을 제시하는 것이므로, 결과를 뒷받침하거나 보충 설명할 수 있는 모든 관찰을 서술.

##### Conclusion
- After feedback

### Apendix
- 표 A1: LexGLUE 데이터의 레이블로 사용된 10개 법률 조항과 각 조항의 의미 설명
- 표 A2: 판사가 판결 시 참조하는 10개 조항별 요건사실표 (각 조항마다 필요한 사실요소들의 목록)
- 표 A3: LexGLUE 데이터로부터 DialFact 형식의 claim을 생성하기 위해 참조한 조항별 후보 문장 5개씩 (총 50개 문장)

---
### Checklist
[ ] 참고문헌 링크 및 서식 점검
	레퍼런스 링크 확인: 논문에 인용한 모든 참고문헌의 하이퍼링크를 하나씩 클릭해 보고, 올바른 논문/자료로 연결되는지 확인
	. DOI나 URL이 깨진 건 없는지, 또 각 레퍼런스 번호와 본문 인용 표시가 제대로 매칭되는지 점검하세요. 특히 자동 생성한 BibTeX을 썼다면 저자명, 연도 등이 틀리지 않았는지 확인이 필요합니다.

[ ] 서식 및 형식 검사
	ACL 양식 등 제출 형식 가이드라인에 맞게 논문이 작성되었는지 최종 확인합니다. 예를 들어 폰트 크기, 줄간격, 여백, 문단 들여쓰기, 참고문헌 스타일 등이 요구사항에 부합해야 합니다
	. 형식적인 요구를 지키지 않으면 불이익을 받을 수 있으므로, 제출 규정 문서를 다시 한번 훑어보고 어긋나는 부분이 없는지 살핍니다

[ ] 통일성과 오탈자 검사
	논문 전반에서 용어 사용이 일관적인지 (요건사실표를 어떤 곳에서는 "요건 사실 테이블"로 쓰지 않았는지 등), 표기법이 통일되었는지 확인합니다. 그림이나 표 번호가 중복되거나 빠진 곳은 없는지도 봅니다. 아울러 맞춤법과 문법 오류가 있는지 빠르게 훑어보세요. 문법 검사기를 사용하거나, 문장을 한 번 소리 내어 읽어보면서 부자연스러운 표현을 교정하면 좋습니다. 가능하다면 동료나 지인에게 초안을 보여줘 피드백을 받는 것도 도움이 됩니다.

[ ] 표절 검토: 최종 공유 전에, 논문 초안을 한 번 표절 검사 소프트웨어에 통과시켜 봅니다. 혹시 본인이 인용 표기를 깜빡 잊고 가져온 문장이 있는지, 또는 의도하지 않은 표절 요소가 있는지 점검합니다
. 표절 검사 결과 유사도가 높게 나온 부분이 있다면 해당 문장을 **재구성(paraphrase)**하거나 적절히 인용 표시를 추가해 수정하세요. 특히 Related Work 등에서 여러 문헌을 인용하다 보면 문장이 겹칠 수 있으니 유의합니다.

[ ] 인용 및 참고 완비: 인용한 문헌들은 모두 References에 들어있는지, 반대로 Reference에 있는 문헌은 본문에서 최소 한 번 인용되었는지 확인합니다. 누락된 인용 표시는 없는지 점검하고, 필요시 BibTeX 데이터를 업데이트합니다.

[ ] 윤리 준수 확인: 표절과 더불어 연구 윤리 측면에서 문제될 부분은 없는지 살핍니다. 예를 들어 데이터 출처를 명시해야 한다면 했는지, 사용한 코드가 남의 코드를 차용한 경우 라이선스를 어겼지는 않은지 등도 확인합니다. 논문 투고 전 이러한 윤리 사항을 모두 점검하여 연구 부정행위가 없도록 합니다.

